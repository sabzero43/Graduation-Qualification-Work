{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы знаем как представить текс в виде длинного разреженного вектора частот слов. Теперь научимся на основе такого вектора принимать решения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано:\n",
    "<ul>\n",
    "    <li>Коллекция документов по тематике - положительные примеры</li>\n",
    "    <li>Фоновая коллекция документов - отрицательные примеры</li>\n",
    "    <li>Длина документа - от нескольких предложений (для коротких текстов есть методы существенно лучше)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно простроить бинарный классификатор:\n",
    "$${Classifier} : \\mathcal{R}^d \\rightarrow \\{0, 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это функция, которая по вещественному вектору d выдает класс 1, если документ относиться к интересующей нас тематике, и 0 для всех остальных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия определяется формулой:\n",
    "$$ \\hat{y}(x) = \\sigma(w^Tx + b)$$\n",
    "Не смотря на то, что модель называется регрессией, предназначена она для классификации\n",
    "\n",
    "$x \\in \\mathcal{R}^d$ - вектор признаков, дополненный фиктивным единичным признаком  \n",
    "$\\hat{y} \\in \\mathcal{R},\\; 0 \\le \\hat{y} \\le 1$ - вероятность выпадения 1, для случайной величины $ y \\in \\{0, 1\\}$  \n",
    "$w \\in \\mathcal{R}^d$  - вектор весов  \n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$ - логистическая <b>функция</b> (сигмоида)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на формулу повнимательнее и попробуем понять, что происходит. Внутри у нас стоит скалярное произведение двух векторов: вектора весов w и вектора признаков x. Результат скалярного произведения — это вещественное число. Если отбросить сигму $\\sigma$, то получится обычная линейная регрессия. Однако у нас задача классификации и мы хотим вероятности. Тогда сожмём выход линейной регрессии с помощью сигмоиды и получим примерно следующий график."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='линейная регрессия и сигмоида.png' alt='график'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Он лежит строго между нулём и единицей. Решение о принадлежности к положительному классу принимается с помощью порога — если предсказанная вероятность больше некоторого числа, то документ относится к положительному классу, и не относится в противном случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настраивать веса этой модели будем, минимизируя значение функции потерь бинарной кросс-энтропии\n",
    "$$ BCE(\\hat{y}, y) = -y \\, log\\hat{y}-(1-y)log(1-\\hat{y}) \\rightarrow min $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Она определяется следующим образом: у нас есть два слагаемых: первое слагаемое отвечает за штраф в случае ложно-отрицательных предсказаний, то есть когда документ принадлежит к положительному классу, а модель выдала для него низкую вероятность. Второе слагаемое отвечает за штраф в случае ложно-положительных предсказаний, и нам нужно минимизировать общий штраф. Данная формула описывает функцию потерь для одного примера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм настройки - градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>!</b> Кросс-энтропия сильно штрафует за сильные ошибки и за неуверенность. На практике это может приводить к тому, что обученная модель чрезмерно \"уверена\" в своих предсказаниях - значения вероятностей становятся близки к 0 или 1. Поэтому использовать предсказания модели как оценку достоверности предсказаний надо с большой осторожностью. В случае с логистической регрессией калибровка (адекватность оценки вероятности) может быть неплохой, но в случае с глубокими нейросетями это чаще всего не так."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
