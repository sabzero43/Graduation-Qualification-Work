{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторная модель документа и TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще этот метод называют метод <b>мешка слов</b> (<b>bag of words</b>).\n",
    "\n",
    "Они хорошо работают, когда класс документа соответствует его тематике. Как правило, тематика документа хорошо описывается составом словаря, который используется в этом документе, а также частотами слов, а не тем, как именно они употребляются в документе, не структурой фраз. Тогда каждый документ описывается длинным вектором размерности порядка десятков или сотен тысяч элементов. Большая часть — нули. Для каждого слова в документе мы имеем какое-то вещественное число.\n",
    "\n",
    "Очевидно, модели должны работать лучше, когда веса слов отличаются — чем значимее слово, тем больше его вес.\n",
    "\n",
    "Рассмотрим несколько вариантов подсчета весов и разберем их приемущества и недостатки.\n",
    "\n",
    "Самый простой вариант - взвесить слово по колличеству его употреблений в документе. Веса слов — это просто целые числа. Естественно, данный подход имеет недостатки. Во-первых, вес слова зависит от длины документа. В длинных документах слова имеют больший вес, как будто бы они более значимы, но это не так. Во-вторых, самые частотные слова — это союзы, предлоги, местоимения... Они встречаются везде, но абсолютно неинформативны и редко бывают полезны для каких-либо задач классификации.\n",
    "\n",
    "Попробуем бороться с этими проблемами. Для начала отнормируем вектор документа на его длину.\n",
    "\n",
    "$$n*w_{i} = \\frac{w_{i}}{\\sqrt{\\sum_{j}w^2_{j}}}$$\n",
    "\n",
    "Теперь веса слов будут зависеть от длинны документа гораздо слабее. Но они все равно будут зависеть, ведь с увеличением длины документа расширается словарный запас. Однако по прежнему предлоги и союзы это самые значимые слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое время рассказать про один из фундаментальных эмпирических законов лингвистики (и не только лингвистики, на самом деле) — закон Ципфа. Возьмём большую коллекцию документов, посчитаем для каждого слова в этой коллекции частоту его встречаемости, то есть количество документов, в которых это слово используется, потом отсортируем полученный список по убыванию частот и получим примерно следующий график."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Закон Ципфа.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По оси абсцисс отложим ранг слова, то есть его порядковый номер в отсортированном списке. По оси ординат отложим частоту слова — относительную частоту. Это график распределения Ципфа — распределения вероятностей, описывающего взаимоотношения частоты события и количества событий с такой частотой. Оно относится к классу степенных распределений и задаётся следующей функцей:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(rank; s; N) = \\frac{1}{Z(s,N)rank^s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank - порядковый номер слова после сортировки по убыванию частоты\n",
    "\n",
    "s - коэффициент скорости убывания вероятности\n",
    "\n",
    "N - количество слов\n",
    "\n",
    "Z - нормализационная константа, чтобы распределение стало распределением\n",
    "\n",
    "$$Z(s, N) = \\sum^N_{i=1}i^{-s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из всего этого можно сделать вывод, что частотных слов мало и они неинформтивны\n",
    "А вот редких слов очень много — если мы какое-нибудь редкое слово встречаем в документе, то мы с большой уверенностью можем сказать, к какой тематике он относится. Но проблема в том, что такие слова очень редки, и поэтому они ненадёжны в качестве факторов при принятии решений.\n",
    "\n",
    "Значит, нам нужно придерживаться баланса частотности и информативности. \n",
    "\n",
    "Баланс частотности и информативности:\n",
    "<ul>\n",
    "    <li>Чаще встречается в документе - более характерен для этого документа</li>\n",
    "    <li>Реже встречается в корпусе - более информативен</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За этот баланс отвечают две величины: TF и IDF\n",
    "\n",
    "TF (term frequency) — это частота слова в документе.\n",
    "$$TF(w, d) = \\frac{WordCount(w, d)}{Length(d)}$$\n",
    "\n",
    "WordCount(w, d) - кол-во употреблений слова ц в документе d\n",
    "\n",
    "Length(d) - длина документа d в словах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " IDF (inverse document frequency) — обратная частота слова в документах.\n",
    " $$IDF(w, c) = \\frac{Size(c)}{DocCount(w, c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DocCount(w, c) - количество документов в коллекции с, в которых встречается слово w\n",
    "\n",
    "Size(c) - размер коллекции с в документах\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда итоговый вес слова можно посчитать, как произведение этих двух величин\n",
    "$$TFIDF(w, d, c) = TF(w, d) * IDF(w, c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике, TF часто логарифмируют. Это позволяет сделать распределение весов слов менее контрастным и уменьшить его дисперсию. Логарифмирование поверх TF полезно, когда в Вашем датасете документы сильно отличаются по длине. В целом логарифмировать IDF тоже можно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм взвешивания признаков по TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Применить нормализацию текста (стемминг или лемматизацию), выделить базовые элементы (символы, токены, n-граммы)</li>\n",
    "    <li>Построить частотный словарь DocCount(w, c) для всех w</li>\n",
    "    <li>Проредить словарь по частоте (выкинуть слишком редкие и слишком частые элементы)</li>\n",
    "    <li>\n",
    "        Для каждого документа d:\n",
    "        <ol>\n",
    "            <li>\n",
    "                Для каждого слова w из d найти WordCount(w, d). <br>\n",
    "                Записать в результирующий вектор в позицию w значение TFIDF (по формуле)\n",
    "            </li>\n",
    "            <li>Записать вектор документа в таблицу признаков документов коллекции</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2* проходимся по всей коллекции документов и, для каждого слова, подсчитываем, в каком количестве документов оно встретилось\n",
    "\n",
    "3* Если у нас большая коллекция, то словарь может получиться просто гигантским, особенно если мы работаем с N-граммами, поэтому периодически, во время построения, или после этой процедуры, мы должны выбросить из словаря всё, что считаем неинформативным\n",
    "\n",
    "4* Строим матрицу признаков. Каждая строчка этой матрицы соответствует документу, а каждый столбец — статистике встречаемости этого слова в документе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF - это способ отбора категориальных признаков (не только в классификации и не только для текстов) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF никак не использует информацию о метке объекта — это одновременно и преимущество, и недостаток. Преимущество заключается в том, что мы можем использовать TF-IDF, не имея меток, то есть задачах обучения без учителя. Недостаток — в том, что мы теряем информацию или недостаточно эффективно её используем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise mutual inforamation (взаимная информация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один способ взвешивания по частоте - взаимная информация. Она измеряется между двумя случайными событиями или реализациями двух случайных величин. Она характеризует, насколько сильнее мы будем ожидать первое событие, если перед этим пронаблюдаем второе (по сравнению с нашими априорными ожиданиями). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pmi(A, B):\n",
    "<ul>\n",
    "    <li>Измеряется между двумя случайными величинами A и B (или реализации двух случайных величин)</li>\n",
    "    <li>Мера того, насколько сильнее мы будем ожидать появление А после наблюдения события B, по сравнению с нашими ожиданиями ситуации, когда событие B мы не наблюдали</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формулы для задачи классификации текстов:\n",
    "$$pmi(I, w) = log\\frac{p(w, I)}{p(w)P(I)} = log\\frac{p(I|w)}{p(I)} = log\\frac{p(w|I)}{p(w)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I - коллекция документов, соответствующая некоторой метке класса L\n",
    "\n",
    "w - слово из словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(w, I) = \\frac{DocCount(w, I)}{Size(I)}$$ - вероятность встретить слово w в документе класса L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(w) = \\frac{\\sum_{I}DocCount(w, I)}{\\sum_{I}Size(I)}$$ - маржинальная вероятность употребления слова w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(I) = \\frac{Size(I)}{\\sum_{m}Size(m)}$$ - маржинальная вероятность встретить документ класса L "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим вот эту формулу: $$pmi(I, w) = log\\frac{p(I|w)}{p(I)}$$  в знаменателе содержится уровень наших априорных ожиданий о появлении события L, а в числителе — уровень ожидания после наблюдения события W. Все три варианта формул на слайде эквивалентны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если возвращаться к текстам, то у нас есть два события. Первое — \"L\": \"мы наблюдаем документ из класса L\". Второе событие — \"W\": \"мы видим в документе слово W\". Все вероятности вычисляются по классическому определению вероятности, то есть как отношение количества положительных исходов к общему числу исходов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взаимная информация — это тоже способ взвешивания и отбора категориальных признаков. В первую очередь, он подходит для задач классификации. В задачах регрессии его тоже можно применять — например, дискретизировав целевое распределение, но это уже сложнее. Он требует наличия двух событий, что усложняет его применение в задачах обучения без учителя, хотя он используется для получения плотных векторных представлений слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
